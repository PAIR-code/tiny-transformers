# A "Named-Dimensions" TypeScript Library for TensorFlowJS

| Status        | Proposed       |
:-------------- |:---------------------------------------------------- |
| **RFC #**     | [395](https://github.com/tensorflow/community/pull/395) |
| **Author(s)** | Lucas Dixon (ldixon@google.com) |
| **Sponsor**   | Ping Yu (piyu@google.com)       |
| **Updated**   | 2021-07-31                      |

## Objective

Support the naming of dimensions in TensorFlow JS to provide an abstraction that makes it easier for developers and researchers to write and read ML algorithms (and support some automated optimisation):

 * Make it harder to make subtle mistakes (have type-checking get intelligible type-error messages).
 * Leveraging TypeScript IDE support to get better auto-completion.
 * Make it easier to support writing an algorithm and then generalise it to support batches.
 * Have code more closely resemble the pseudocode and algorithms in papers.
 * Allow machine optimization of the ordering of indexes, transpositions and permutations.

## Motivation

ML programming is currently much like assembly programming: you have to remember what each index in a tensor means, and refer to dimensions by index numbers, and when they change due to the operations you perform, you have to remember the new index position. Higher level abstractions could make this better and various ideas have been proposed around naming the dimensions in a Tensor e.g. http://nlp.seas.harvard.edu/NamedTensor

The impact is likely largest for people who want to write ML algorithms, and for people using TFJS for education, e.g. to illustrate and visualize ML algorithms. However, it may also improve the experience for others using TFJS by providing better tool support, better auto-completion, and improved  optimization for evaluation.

Here's an example TFJS implementation of single attention-head:

```ts
// keys.shape == [seqLen, inputRep]
const keys = tf.matMul(xs, keyM.read())

// queries.shape == [seqLen, inputRep]
const queries = tf.matMul(xs, queryM.read())

// attention.shape == [seqLen (query), seqLen (key)]
const attention = tf.softmax(
  tf.div(tf.matMul(queries, keys, false, true), inputRepSizeSqrt)) as tf.Tensor2D;

// values.shape == [seqLen, valueRepSize]
const values = tf.matMul(xs, valueM.read())

// attendedValues.shape == [seqLen (query), valueRepSize]
const attendedValues = tf.matMul(attention, values);
```

The comments tell you which tensor index position corresponds to which dimension, and the author has to keep track of them and add appropriate comments to understand the code. It's hard to see the errors, especially without comments. For example, the following is very similar, will not typically result in a runtime error, but will give the wrong result:

```ts
const keys = tf.matMul(xs, keyM.read())
const queries = tf.matMul(xs, queryM.read())
const attention = tf.softmax(
  tf.div(tf.matMul(queries, keys, true, false), inputRepSizeSqrt)) as tf.Tensor2D;
const values = tf.matMul(xs, valueM.read(), true, false)
const attendedValues = tf.matMul(attention, values, false, true);
```

The proposed Named-Dimension library allows the same thing to be written as:

```ts
const inputKeys = xs.inputRep.dot(keyM.inputRep).seqLen.rename('keySeqLen');
const inputQueries = xs.inputRep.dot(queryM.inputRep);
const attention = inputKeys.kqRep.dot(inputQueries.kqRep);
const values = xs.inputRep.dot(valueM.inputRep);
const attendedValues = values.seqLen.dot(attention.seqLen).keySeqLen.rename('seqLen');
```

This has the following advantages:
* It's easy to see what dimensions are being multiplied together: this is now explicit where it was previously implicit.
* TypeScript editors, like VSCode, support auto-completeion to select only the valid dimensions. Referencing an index that does not exist can no longer happen. e.g. auto-completion of `xs.` results in `inputRep` and `seqLen`. No need to remember which order they have either.
* The optimal ordering of indexes can be computed behind the scenes, so the user does not need to worry about transpositions and permutations, or how to make it optimal.

## User Benefit

A headline might be: "ML programming in JS just got much easier with Named Dimensions". Programming is easier in two import ways:

1. It's easier to write code (and harder to write subtle bugs). This is because there is helpful auto-completion, one doesn't need to remember the dimensions, and you can no longer accidentally multiply the wrong dimensions together (you can do this only via an explicit rename operation).
2. And It's easier to read code too: one doesn't need to remember and map the indexes to what they mean.
3. You don't need to work hard to find the optimal ordering and calling of permutation operations. This gets done for you behind the scenes.

This combines many of the benefits of eager and lazy evaluation.

## Design Proposal

The key idea is to introduce a Named-Dimension layer over the TFJS Tensor object. This maintains a mapping between the indexes and their corresponding names. We leverage TypeScript's ability to have a type that is a set of string literals (within union operators `|` ). Using TypeScripts intersection (`&`) and exclusion (`Exclude`) type operations, we can automatically infer types appropriately. We can also use the type choice operator (`?`) and introduce some unique error types to make error messages easy to read and act uppon.

A `GTensor` (a "Graph Tensor", inspired by [OpenGraphs](https://arxiv.org/abs/1011.4114)) is a tensor with a set of named dimensions parameters (each dimension is an axis in the larger tensor). Each named dimesion parameter is just the literal string type of the name.

Each `GTensor` object has a `dim` property that gives access to the dimensions as named properties. Each dimension is parameterized by both the `GTensor` it comes from, as well as the name of the type it is a dimension of. To select which dimension to perform an operation on, one simply uses the property of that dimension.

```ts
const g1: GTensor<'inputTokens'|'tokenRep'> = ...;
const g2: GTensor<'tokenRep'|'queryRep'> = ...;
const g3 = g1.dim.inputTokens.dot(g2.dim.inputTokens);
// g3: GTensor<'inputTokens'|'queryRep'>
```

Type-checking ensures that dimension names match. i.e.

```ts
g1.dim.inputTokens.dot(g2.dim.foo)
              // Type Error: "inputTokens" is different from "foo".
```

Dimensions can be renamed also to provide a new dimension object with the correct name, e.g. if `g2` didn't have an `inputTokens` dimension, but we wanted to dot product with the `foo` dimension, we could do:

```ts
g1.dim.inputTokens.dot(g2.dim.foo.rename('inputTokens'));
```

By working at this more abstract level, you never need to worry about the axis position, you just reference it by name. Underneath this abstraction, we can now optimise the "layout" of the tensors (the order of the axis) and the various permutation operations.

The vision is that this also provides a higher level abstraction that can be used to efficiently compile to XLA, and thus provide a better high level abstraction for ML programming in TypeScript, with better tool support, making it easier for more people be able to explore and write ML algorithms, and remove a large part of the boring and frustrating challenges of making sure indexes align correctly. A side effect is that this also makes code much more readable (see the Attention Head implementation below).

We can make a `GTensor` using an initializer, or by naming the dimensions in a tf.Tensor (and potentially just providing a array or typed array directly):

```ts
   // Making a GTensor with an initializer:
   const g1 = gtensor.makeTruncNormal({ inputRep: 2, kqRep: 3 });
   // gshape() gives you the dict that describes the dimension's sizes.
   expect(g1.gshape()).toEqual({ inputRep: 2, kqRep: 3});

   // Making a GTensor from a tensor by naming the dimensions:
   const g2 = new gtensor.GTensor(tf.tensor(
         [ [ // 'example' dimension index 0
             [ // 'pos' dimension index 0: contains an array of repSize
               1, // 'repSize' dimension index 0
               2  // repSize index 1
             ],
             [3, 4], // pos index 1
             [5, 6], // pos index 2
           ],
           [ [1, 2], [3, 4], [5, 6], ], // example index 1
           [ [1, 2], [3, 4], [5, 6], ], // example index 2
           [ [1, 2], [3, 4], [5, 6], ], // example index 3
         ]
         ), ['example', 'pos', 'repSize']);
   expect(g2.gshape()).toEqual({example: 4, pos: 3, repSize: 2});
```

TypeScript's type-inference for `g1` (e.g. in VSCode, mouse-over g1) is:

```ts
const g1: gtensor.GTensor<"inputRep" | "kqRep">
```

And for `g2` it is:

```ts
const g2: gtensor.GTensor<"example" | "pos" | "repSize">
```

We treat each dimension as a first class object (not just string-indexed members of a dictionary): one can access the dimensions of a `GTensor` via the property `dim`, and each dimension of a `GTensor` is a property itself. This allows for nice auto-completion, in particular, for the above example, `g1.dim.` auto-completes with the drop down menu with `inputRep` first and `kqRep` second. Notice that the type was correctly inferred from the object given to the constructor.

Because referencing by `.dim.` many times becomes tedius, we suggest working with the named dimensions object directly. A `Dimension` knows the `GTensor` it is part of and this is reflected in the Dimension's type which has two parameters: one being the set of dimensions in the `GTensor`, and the other the selected dimension.

This allows the key part of an attention head to be programmed as follows:

```ts
   function attentionHeadFn(input: Dims<'seqLen'|'inputRep'>)
  	: Dims<'seqLen'|'valueRep'> {
     const inputKeys = input.inputRep.dot(keyM.inputRep)
                            .seqLen.rename('keySeqLen');
     const inputQueries = input.inputRep.dot(queryM.inputRep);
     const attention = inputKeys.kqRep.dot(inputQueries.kqRep);
     const values = input.inputRep.dot(valueM.inputRep);
     const attendedValues = values.seqLen.dot(attention.seqLen)
                                  .keySeqLen.rename('seqLen');
     return attendedValues;
   }
```

Where `Dims` is the convenient type for the `dim` object of a `GTensor`; this is what avoids having to write `.dim.` everywhere, and instead to be able to directly treat dimensions as properties of an object.

Because batching is a key and often complex part of programming, we note that this paradigm makes turning a non-batched computation into a batched one somewhat "beautiful":

```ts
   const batchedAttentionHeadFn = gtensor.liftFnOverDim(
     'batchSize', attentionHeadFn);
   const batchedAttendedValues = batchedAttentionHeadFn(batchedInput);
```

One simply has to specify the name of the batch dimension.

A prototype implementation of all of this is included ([`gtensor.ts`](./gtensor.ts) and [`gtensor.spec.ts`](./gtensor.spec.ts))

### Alternatives Considered

#### Dimensionsets

In future work, we think we should generalise "Dimension" to being a "Subdimensions" that can reference many axis names.

#### What about including the dimension size in the type?

We did expriment with including the size as an integer in the type, however, we found that this makes the types much longer, more difficult, and the type-handling code is far more complicated. Overall, because a name has a fixed size, the only benefit we say to doing this is that it would catch rename operations for incompatible names. Such a class of error is actually quite easy to spot, and typically also caught by any tests on the first execution run.

### Performance Implications

* Do you expect any (speed / memory)? How will you confirm?

It's expected that the overall performance of code written using this API will
be faster, depeneding on the complexity of the optimization of permutation
operations. But because it's just a way to organize the computation, the final
computation, or an equivalently manually optimised algorithm will not be
different.

For eager execution code, we expect a very slow down due to the extra
book-keeping of named dimensions.

* There should be microbenchmarks. Are there?

For code run on the GPU there is no impact. For code run on CPU eagerly, we do
not yet have microbenchmarks, but if this RFC proves to interesting to the
community we will write some.

* There should be end-to-end tests and benchmarks. If there are not (since this is still a design), how will you track that these will be created?

There are some tests, this is still relatively early stage (a full
implementation is not complete). But tests and benchmarks will be tracked with
github issues.

### Dependencies
* Dependencies: does this proposal add any new dependencies to TensorFlow?

The proposed library depends on TensorFlow JS only.

### Engineering Impact
* Do you expect changes to binary size / startup time / build time / test times?

This is expected to start as a separate library, with no affect on existing TF
code.

* Who will maintain this code? Is this code in its own buildable unit? Can this code be tested in its own? Is visibility suitably restricted to only a small API surface for others to use?

This can be tested on it's own, and the long term plan would be that it is maintained and developed by the TFJS team.

### Platforms and Environments
* Platforms: does this work on all platforms supported by TensorFlow? If not, why is that ok? Will it work on embedded/mobile? Does it impact automatic code generation or mobile stripping tooling? Will it work with transformation tools?

This is an extension of TFJS, and while it can work in pure JS, key advantages (e.g. auto-completion, and type-checking) will only apply to those working in TypeScript.

* Execution environments (Cloud services, accelerator hardware): what impact do you expect and how will you confirm?

No impact. In time one would imagine a coimpilation from NamedTensors to XLA. This would allow TFJS code to directly on accelerators.

### Best Practices
* Does this proposal change best practices for some aspect of using/developing TensorFlow? How will these changes be communicated/enforced?

Not in the short term. It might after further experimentation be a better way to write TFJS code (being more readable), and may in time become the standard for that (in the short term, NamedTensor code would look different to other tensorflow code, although it is we believe more readable).


### Tutorials and Examples
* If design changes existing API or creates new ones, the design owner should create end-to-end examples (ideally, a tutorial) which reflects how new feature will be used. Some things to consider related to the tutorial:
    - The minimum requirements for this are to consider how this would be used in a Keras-based workflow, as well as a non-Keras (low-level) workflow. If either isn’t applicable, explain why.

    See the [included exampe of implementing Transformers in TFJS]().

    - It should show the usage of the new feature in an end to end example (from data reading to serving, if applicable). Many new features have unexpected effects in parts far away from the place of change that can be found by running through an end-to-end example. TFX [Examples](https://github.com/tensorflow/tfx/tree/master/tfx/examples) have historically been good in identifying such unexpected side-effects and are as such one recommended path for testing things end-to-end.

    We've like early feedback before implementing a lot more examples. But happy to do more of these if this proposal receives interest.

    - This should be written as if it is documentation of the new feature, i.e., consumable by a user, not a TensorFlow developer.
    - The code does not need to work (since the feature is not implemented yet) but the expectation is that the code does work before the feature can be merged.

### Compatibility
* Does the design conform to the backwards & forwards compatibility [requirements](https://www.tensorflow.org/programmers_guide/version_compat)?

Yes.

* How will this proposal interact with other parts of the TensorFlow Ecosystem?
    - How will it work with TFLite?
    - How will it work with distribution strategies?
    - How will it interact with tf.function?
    - Will this work on GPU/TPU?
    - How will it serialize to a SavedModel?
  For all these there is no change w.r.t. TFJS.

### User Impact
* What are the user-facing changes? How will this feature be rolled out?

This would start as a new TFJS library. The long term vision is to provide a better way to write TFJS code that leverages modern IDEs and produces more readable, efficient, and less error prone code. Maybe, longer term, it will set a standard for other parts of the TF ecosystem. But we want to start with a smaller library and experiment with it. TFJS is the best part of the ecosystem for this experiment as we can leverage the flexible and strong TypeScript type system and modern IDE support.

## Detailed Design

This section is optional. Elaborate on details if they’re important to
understanding the design, but would make it hard to read the proposal section
above.

## Questions and Discussion Topics

Seed this with open questions you require feedback on from the RFC process.

* We have not looked hard at the permutation optimization code, pointers or
  remarks on teh difficulty of this and its potential impact would be
  particularly welcome.

* We think the typing code could be simplified with a small amount of support
  for the TypeScript team. We havn't explored this but would welcome pointers or
  thoughts about this.

* If there was interest in have XLA compilation, then this might provide a
  competative framework for writing larger scale models in TypeScript. This
  feels like it could be particularly interesting for the nodejs ML community.

* There is a tradeoff with complexity of types that get checked vs
  understandability of type error messages. Currently type errors are very easy
  to understand. Mis-matched types result in a type-error that says what is
  mis-matches, and gives the missing/extra name. e.g. Using strings for named
  dimensions doesn't actually check the dimension's size, but gives a nice
  compromise between type-checking, readability, and clear type-error messages.
  But it may be possible to get the dimensions included also, but it'll make the
  types less readable. Note: the TFJS team did previous try to include the
  specify the specification of the size of each dimension in the TypeScript
  Tensor type (e.g. Tensor<Rank.4D, Dtype.Float32>), however, it resulted in
  very hard to comprehend type errors, and was removed.

  That attempt did not try to include the name of a dimension. Thus different
  dimensions with the same size, in the old effort, could still be confused
  easily (because remembering the meaning of dimension and which index it
  corresponds to is easy to forget).

  Moreover, in the earlier effort, the type system only had index positions and
  their size to create error messages with, thus having readable error messages
  was very challenging.

* There is a tradeoff between high level specification vs control of exactly
  what a computation gets performed. The proposal here takes away the low level
  control of dimension ordering, and the need to control the memory layout that
  way. The hypothesis is that, like in machine code, having to remember stack
  indexes isn't something people do well, and using names and having that
  automated will be a better compromise.

* We should explore splitting and merging of dimensions.

* What's the relationship between the Layers API and GTensor?

  The Layers API is important for TFJS, so the relationship to `GTensors` should
  be elaborated and explored beyond the obvious: every `GTensor` contains a tensor internally, so Layers can be applied to it, and conversely the output of any layers operation can be turned into a `GTensor`.

  It is worth noting that the `Lifting` operation on `GTensors` provides a
  simpler and more direct alternative to `batch` functionality of Layers. So it
  may be that `GTensors` essentially replace the `Layers` API, but that would take some work to make `GTensor` equivalents for all the functionality in the Layers API.

* Should we just make Tensor always be "named" (aka a GTensor)? There's a lot of
  code that depends on not having names. One option we could consider is that
  Tensor can optionally have names. And if they do, then named stuff "just
  works". This might introduce more complexity where having a separate simpler
  library might be easier to work with.

* Bridge to EINSUM/XMAP notation? Can GTensor provide a DSL for EINSUM/XMAP
  operation?

## Related Reading:

* Harvard NLP PyTorch Named Tensors:
  * Motivation: Tensors Considered Harmful (Harvard NLP online article)
  * Docs: https://pytorch.org/docs/stable/named_tensor.html
  * Code: https://github.com/harvardnlp/namedtensor/

* XArray: pandas inspired numerical library for python.
  * Docs: http://xarray.pydata.org/en/stable/

* Haskell:
  * Dex: https://google-research.github.io/dex-lang/prelude.html
  and https://github.com/google-research/dex-lang
  * Using Dependent Types: https://blog.jle.im/entry/practical-dependent-types-in-haskell-1.html

* NamedTensor for better ML notation
  * https://namedtensor.github.io/ and Figure 4 / Algo. 2 of https://arxiv.org/pdf/2006.16668.pdf
